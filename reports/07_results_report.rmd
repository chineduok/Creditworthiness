---
output: 
  pdf_document: 
    fig_caption: yes
    number_sections: no
---

#  BUSINESS REPORT- PREDICTING DEFAULT RISK
**************
## Section 1: Business Understanding

__Business Situation__  
Our bank receives 200 loan applications per week, but due to a financial scandal that hit a competitor the credit risk unit of the bank will be processing 500 applications this week.The influx of new credit applications is a great opportunity the bank wants to immediately pursue.

__The Complication__  
The bank will want to maintain there processing turnaround time while ensuring that the credit risk unit is able to effectively determine creditworthy applications, while reducing the risk of default by effectively determining non-creditworthy applications.

__Key Decision that needs to be made__  
The Head of the credit risk department needs to decide if a loan should be approved for each of the 500 loan applications received this week.

__Approach__  
This project is data rich; it has readily available information that can be used to predict creditworthiness of the 500 loan applications. The data will be acquired internally from already processed loan applications,'customers-to-score' and the data from the 500 loan applications yet to be reviewed,'customers-to-score'. The two sets of data include personal details about the customer, such as their age and how long they have been at their current job. It will also include details on the individual’s banking and credit history, such as their account balance, number of credits at this bank, and their payment status of previous credit.

We will use the data set with already processed loan application to build a binary classification predictive model from 4 different algorithms to determine if a customer is creditworthy or non-creditworthy. We will then make some comparisons with some performance metric to determine the algorithm that will be used to predict the Creditworthiness of the 500 new loan applications. 




## Section 2: Data Structure & Quality

```{r Load Libraries & Import Data Sets, message=FALSE, warning=FALSE, include=FALSE}
library(rio)
library(Amelia)
library(tidyverse)
library(ggplot2)
library(dplyr)
library(gridExtra)
library(caret)
library(rpart)
library(gbm)
library(randomForest)
library('devtools')
library(woe)
library(ResourceSelection)
library(ROCR)

```

The data we used to train the model was an equivalent sum of 500 loan applications with 19 variables that includes the outcome variable. The 19 variables included in the data set are listed as follows:
```{r message=FALSE, warning=FALSE, include=FALSE}
train <- import('C:/Users/nedu_/git-projects/Creditworthiness/data/credit-data-training.xlsx')
test <- import('C:/Users/nedu_/git-projects/Creditworthiness/data/customers-to-score.xlsx')
colnames(train)<-make.names(colnames(train))
train$Credit.Application.Result <-make.names(train$Credit.Application.Result)
colnames(test)<-make.names(colnames(test))
```


```{r echo=FALSE}
names(train)
```
_Table 2.1 List of variables_

We checked the data structure and quality to check for missing values.

```{r echo=FALSE, message=FALSE, warning=FALSE}
na_plot<-missmap(train, legend = T, col = c('yellow','black'), main = "Creditworthiness Missingness Map")


```
_Fig 2.1 Missing Values_


```{r echo=FALSE}
train_NA<-data.frame(colSums(is.na(train)))
colnames(train_NA)<-'NAs'
train_NA
```
_Table 2.2 NA's per variable_

The missing values plot showed that the variables _Duration.in.Current.address_ had more than 50% of its values missing and _Age.years_ had less than 5% missing values. We dropped _Duration.in.Current.address_ and imputed the missing _Age.years_ with the median value.Character variables were also encoded as factors.

```{r include=FALSE}
train<-train%>%select(-Duration.in.Current.address)

for (i in (1:length(train$Age.years)))
{
  if(is.na(train$Age.years[i])==T){
    train$Age.years[i]<- median(train$Age.years,na.rm = T)}
  else{
    train$Age.years[i]
  } }


## Factor Encoding
train$Account.Balance<- factor(train$Account.Balance)

train$Payment.Status.of.Previous.Credit<-factor(train$Payment.Status.of.Previous.Credit)

train$Purpose <- factor(train$Purpose)

train$Concurrent.Credits <- factor(train$Concurrent.Credits)

train$Occupation <- factor(train$Occupation)

train$Length.of.current.employment<- factor(train$Length.of.current.employment)

train$Guarantors <- factor(train$Guarantors)

train$No.of.Credits.at.this.Bank<-factor(train$No.of.Credits.at.this.Bank)

train$Value.Savings.Stocks<-factor(train$Value.Savings.Stocks,levels = c("None","< £100","£100-£1000"))

train$Foreign.Worker<- factor(train$Foreign.Worker)

train$Credit.Application.Result<- factor(train$Credit.Application.Result)

train$Type.of.apartment<- factor(train$Type.of.apartment)

train$Most.valuable.available.asset<-factor(train$Most.valuable.available.asset)


summary(train) 

```




## Section 3: Exploratory Data Analysis

Next we performed some exploratory data analysis to generate some insights from our internal data. We started with some distributions of categorical variables to get a sense of variables that have zero variance or near zro variance that can affect our modeling.

```{r near zero variance, echo=FALSE, fig.height=8, fig.width=10, message=FALSE, warning=FALSE}
nzv.check.plt <- train%>%select_if(is.factor)%>%
  gather()%>%
  ggplot(aes(value))+
  facet_wrap(~key,scales = 'free') +
  geom_bar(show.legend = F)
nzv.check.plt
```
_Fig_3.1 Bar charts of categorical variables_

We can identify immediately 3 variables, _Concurrent.Credits_, _Foreign.Worker_, _Guarantors_ and _Occupation_. To be sure of this we calculate the frequency ratio of the most occurrence over the second most occurrence value within the variables and also the percent unique to check their validity.

```{r echo=FALSE}
nzv <- nearZeroVar(train, saveMetrics = T)

nzv%>%filter(nzv==T)

# Drop variables with the zero or near zero variance predictors and telephone number
nzv<-nearZeroVar(train)
train_new<-train[,-nzv]
```
_Table 3.1 Near Zero Variance predictor variables_

We discovered that only three variables meet this criteria as shown in Table 3.1 above and these variables were dropped. We continue with our analysis by plotting the distribution of the continuous variables.

```{r message=FALSE, warning=FALSE, include=FALSE}
dist.plt1 <- train%>%
  ggplot(aes(Credit.Amount, fill= Credit.Application.Result)) +
  geom_density(alpha=0.5, color ='black') +
  scale_fill_brewer(palette = 'Dark2') +
  scale_x_log10()+
  xlab('Credit Amount') + ylab('Frequency')+
  theme_bw()

dist.plt2 <- train%>%
  ggplot(aes(Age.years, fill= Credit.Application.Result)) +
  geom_density(alpha=0.5, color ='black') +
  scale_fill_brewer(palette = 'Dark2') +
  scale_x_log10()+
  xlab('Age (years)') + ylab('Frequency')+
  theme_bw()

dist.plt3 <- train%>%
  ggplot(aes(Duration.of.Credit.Month, fill= Credit.Application.Result)) +
  geom_density(alpha=0.5, color ='black') +
  scale_fill_brewer(palette = 'Dark2') +
  scale_x_log10()+
  xlab('Duration of Credit (Months)') + ylab('Frequency')+
  theme_bw()

#corresponding boxplots

bx.plt1 <- train%>%
  ggplot(aes(Credit.Amount, fill= Credit.Application.Result)) +
  geom_boxplot(color ='black') +
  scale_fill_brewer(palette = 'Dark2') +
  scale_x_log10()+
  xlab('Credit Amount') + ylab('Frequency')+
  theme_bw()

bx.plt2 <- train%>%
  ggplot(aes(Age.years, fill= Credit.Application.Result)) +
  geom_boxplot(color ='black') +
  scale_fill_brewer(palette = 'Dark2') +
  scale_x_log10()+
  xlab('Age (years)') + ylab('Frequency')+
  theme_bw()

bx.plt3 <- train%>%
  ggplot(aes(Duration.of.Credit.Month, fill= Credit.Application.Result)) +
  geom_boxplot(color ='black') +
  scale_fill_brewer(palette = 'Dark2') +
  scale_x_log10()+
  xlab('Duration of Credit (Months)') + ylab('Frequency')+
  theme_bw()

```

```{r echo=FALSE, message=FALSE, warning=FALSE}
grid.arrange(dist.plt1, dist.plt2, dist.plt3)
```
_Fig 3.2 Density Plots_

We can see that they are not normally distributed and it seems as if the central tendencies for each category in the three plots are different. We also see that the categories follow the same skew direction. The defined humps in Credit Amount and Duration of Credit were there are 3 suggests that there are about 2-3 defined groups in the loan applicants which will be verified with categorical plots subsequently. We also sense that these be strong predictors and this will be further verified with boxplots to visualize the difference in their means.


```{r echo=FALSE, message=FALSE, warning=FALSE}
grid.arrange(bx.plt1, bx.plt2, bx.plt3)
```
_Fig 3.3 Box Plots_

We discover that the difference is means for the two categories are evident for the 3 continuous predictor variables and Duration of Credit is the strongest of the three. This was tested using the t-test and their strength measured by the p-values is shown in the table below. 

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(broom)
strength.predictors <- train%>%select(Credit.Application.Result, Credit.Amount,
                                      Duration.of.Credit.Month, Age.years)%>%
  gather(-Credit.Application.Result, key = "var",value = "value")%>%
  group_by(var)%>%
  do(tidy(t.test(value~ Credit.Application.Result, data=.)))

strength.predictors%>%select(p.value)
```
_Table 3.2 Strength of continuous predictor variables_

Here we look for insights on some of the categorical variables by plotting some bar graphs.

```{r echo=FALSE, fig.height=8, fig.width=8}
b.plt1 <- train%>%
  ggplot(aes(Account.Balance, fill= Credit.Application.Result ))+
  geom_bar(color ='black')+
  scale_fill_brewer(palette = "Accent")+
  xlab('Account Balance') +
  geom_text(stat = "count", aes(label = ..count..),
            inherit.aes = TRUE,position = position_stack(), vjust = 1.5) +
  theme_bw()


b.plt2 <- train%>%
  ggplot(aes(Payment.Status.of.Previous.Credit, fill=Credit.Application.Result))+
  geom_bar(color ='black')+
  scale_fill_brewer(palette = "Accent")+
  xlab('Payment Status of Previous Credit') +
  geom_text(stat = "count", aes(label = ..count..),
            inherit.aes = TRUE,position = position_stack(), vjust = 1.5) +
  theme_bw()

# It will be interesting to get some more insights on this variable beyond the outcome variable

b.plt2a <- train%>%
  ggplot(aes(Purpose, fill=Credit.Application.Result))+
  geom_bar(color ='black')+
  scale_fill_brewer(palette = "Accent")+
  xlab('Purpose') +
  geom_text(stat = "count", aes(label = ..count..),
            inherit.aes = TRUE,position = position_stack(), vjust = 1.5) +
  theme_bw()

grid.arrange(b.plt1, b.plt2, b.plt2a)


```
_Fig 3.4 Barplots group 1_

__Some observations from Fig 3.4__  
__Account Balance__: there are more Creditworthy people with _Some Balance_ than _No Account_ and consequently less people Non.Creditworthy people with _Some Balance_ than _No Account_ . It suggests that having some amount in your account may determine creditworthiness.

__Payment Status of previous Credit__:This chart suggests that while there may be 25 accounts with _Some Problems_ that are classified as Non.Creditworthy, having probably defaulted, there were 75 accounts that are now _Paid Up_ but classified as Non.Creditworthy. The predictive model should be able to reduce this risk with an acceptable mis-classification rate.

__Purpose__: The bank has a bigger appetite for _Home Related_ loans.

```{r echo=FALSE, fig.height=6, fig.width=8}
b.plt3 <- train%>%
  ggplot(aes(Value.Savings.Stocks, fill=Credit.Application.Result))+
  geom_bar(color ='black')+
  scale_fill_brewer(palette = "Accent")+
  xlab('Value in Savings or Stocks') +
  geom_text(stat = "count", aes(label = ..count..),
            inherit.aes = TRUE,position = position_stack(), vjust = 1.5) +
  theme_bw()



b.plt4 <- train%>%
  ggplot(aes(Length.of.current.employment, fill=Credit.Application.Result))+
  geom_bar(color ='black')+
  scale_fill_brewer(palette = "Accent")+
  xlab('Length of current employment') +
  geom_text(stat = "count", aes(label = ..count..),
            inherit.aes = TRUE,position = position_stack(), vjust = 1.5) +
  theme_bw()

grid.arrange(b.plt3,b.plt4)
```
_Fig 3.5 Bar plots group2_

This group of charts seem to follow the same trend. It shows that people with less than a year in current may not have attained an acceptable credit score and without any savings further supports the suggestion that these may be new employees or those just starting out in their careers.




## Section 4: Training the Model

We used 4 different algorithms to train the binary classification model and compared their ROC and Accuracy performance of metrics to determine the best performance for scoring the 500 new loan applications.The algorithms we used are: 

1.Logistic Regression  
2.Decision Tree  
3.Random Forest  
4.Boosted Tree  

We did a train/test split of 70/30 for our external validation and set the hyper parameters to tune the different algorithms using repeated cross validation of 10 for to determine the accuracy of the models and class probability with two class summary to measure the ROC of the models.
```{r Data split, message=FALSE, warning=FALSE, include=FALSE}
set.seed(123)
##Data split
train_index <- createDataPartition(train_new$Credit.Application.Result,times = 1, p = 0.7, list = F)
train_set <- train_new[train_index,]
test_set <- train_new[-train_index,]
```

```{r hyperparameter tuning, message=FALSE, warning=FALSE, include=FALSE}
## Setting the tuning parameters
fitControl <- trainControl(## 10-fold CV
  method = "repeatedcv",
  number = 10,
  #classProbs = T,
  ## repeated ten times
  repeats = 10)

fitControl2<- trainControl(## 10-fold CV
  method = "repeatedcv",
  number = 10,
  classProbs = T,
  ## repeated ten times
  repeats = 10,
  summaryFunction = twoClassSummary)

tune.Grid <-  expand.grid(interaction.depth = c(1, 5, 9), 
                          n.trees = (30:500) ,
                          shrinkage = 0.1,
                          n.minobsinnode = 15)
```

__1. Logistic Regression__

The model gave an accuracy of 73% with a 10-fold repeated cross validation as can be see in the model output below.

```{r GLM by accuracy metric, echo=FALSE, message=FALSE, warning=FALSE}
glmFit <- train(Credit.Application.Result ~ .,
                method = 'glm',
                trControl = fitControl,
                tuneLength =10,
                data = train_set)
#glmFit$finalModel
#summary(glmFit)
glmFit
```
_Table 4.1 Logistic Regression model Accuracy measure output_


We measured the performance of the Logistic Regression using the ROC metric with the output of 74.5% as seen below.

```{r GLM by ROC, echo=FALSE, message=FALSE, warning=FALSE}
#ROC as performance metric
glmFit2 <- train(Credit.Application.Result ~ .,
                method = 'glm',
                metric = 'ROC',
                trControl = fitControl2,
                data = train_set)

glmFit2

```
_Table 4.1b Logistic Regression with ROC measure output_

__Logistic Regression test prediction and confusion matrix__  
We applied the model to the hold-out sample and got an overall of 79.2% as seen in the Confusion Matrix and its statistics output below. This was an improvement on the accuracy of the model. However, the calculated Positive Predictive Value (PPV) of 81.5% and Negative Predictive Value (NPV) of 70.37% showed that the model is biased towards Creditworthy.

```{r Logistic Regression Cm,echo=FALSE, message=FALSE, warning=FALSE}
glm.pred <- predict(glmFit,test_set)
cm<-confusionMatrix(table(glm.pred,test_set$Credit.Application.Result))   
cm
```
_Table 4.2 Confusion Matrix and Statistics output_


__2. Decision Tree__

We used the Decision Tree algorithm to train the model and got an accuracy of 73.27% at a complexity parameter of 0.0375. This can be seen in the line plot of the tuning parameter in Fig 4.2 below.

```{r Dtree by Accuracy metric, echo=FALSE, message=FALSE, warning=FALSE}
treeFit <- train(Credit.Application.Result ~.,
               method = "rpart",
               trControl = fitControl,
               tuneGrid = data.frame(cp = seq(0.0, 0.1, len = 25)),
               data = train_set)

plot(treeFit, main = "Decision Tree-Model Accuracy Plot")
```
_Fig 4.2 Decision Tree complexity parameter tuning plot_


```{r eval=FALSE, include=FALSE}
library(rpart.plot)
prp(treeFit$finalModel)
```


__Decision Tree variable importance__  
We can also see from the variable importance plot that _Payment.Status.of Previous.Credit_ is the most important followed by _Duration.Credit.Month_ and _Account.Balance_ compared to the Logistic Regression model that showed _Account.Balance_ as the most significant predictor.

```{r DTree VarImp, echo=FALSE, message=FALSE, warning=FALSE}
plot(varImp(treeFit), main = "Decision Tree Variable Importance")
```
_Fig 4.3 Decision Tree Variable importance plot_


__Decision Tree model with ROC as metric__  
Next we tuned the Decision Tree model using the ROC metric. This gave us the optimal ROC of 69.39% at a complexity parameter value of 0.03333333. See the ROC Vs CP plot in Fig 4.4 below.

```{r Dtree by ROC, echo=FALSE, message=FALSE, warning=FALSE}
#####ROC metric
treeFit2 <- train(Credit.Application.Result ~.,
                 method = "rpart",
                 trControl = fitControl2,
                 metric ='ROC',
                 tuneGrid = data.frame(cp = seq(0.0, 0.1, len = 25)),
                 data = train_set)

plot(treeFit2,main = "Decision Tree- ROC plot")
```
_Fig 4.4 Decision Tree ROC plot_


__Decision Tree test prediction and confusion matrix__  
Applying the model with accuracy as metric of choice to the hold-out sample gave an overall accuracy of 78.52%. This mode did not perform better than the Logistic Regression.Similar to the Logistic Regression, it is biased towards the Creditworthy category from the values of the PPV and NPV.

```{r Dtree Test prediction and CMx, echo=FALSE, message=FALSE, warning=FALSE}
tree.pred <- predict(treeFit,test_set)
cm2<-confusionMatrix(table(tree.pred,test_set$Credit.Application.Result))   
cm2
```
_Table 4.3 Decision Tree Confusion Matrix Output_


__3. Random Forest__

The Random Forest model showed better accuracy than the first two models. The model gave an accuracy of 75.82%  with 4 as the optimal no of randomly selected predictors. This can also be seen in the Accuracy Plot show in Fig 4.5

```{r RandomForest accuracy, echo=FALSE, message=FALSE, warning=FALSE}
rfFit<- train(Credit.Application.Result ~.,
              method = 'rf',
              trControl = fitControl,
              tuneLength = 10,
              data = train_set)

rfFit
```
_Table 4.4 Random Forest Model (metric=Accuracy)_

```{r RF accuracy plot,echo=FALSE}
plot(rfFit, main = "Random Forest - Accuracy Plot")
```
_Fig 4.5 Random Forest- Accuracy Plot_

__Random Forest variable importance__  
Unlike the Decision Tree that showed the first 3 variables of importance as _Payment.Status.of Previous.Credit_, _Duration.of.Credit.Month_,and _Account Balance_; the Random Forest Model  showed _Account.Balance_, _Payment.Status.of Previous.Credit_ and _Duration.of.Credit.Month_  as the most significant predictors.

```{r RF Variable Importance Plt, echo=FALSE}

plot(varImp(rfFit,10),main = "Random Forest -Variable Importance Plot")
```
_Fig 4.6 Random Forest Variable Importance Plot_

__Random Forest with ROC as metric__   
We used the ROC metric to select the optimal model for the Random Forest algorithm. This gave us the optimal ROC of 74.91% with 2 as the value of randomly selected predictors. We can see the plot of values for the ROC against 10 different randomly selected predictors in Fig 4.6 below.

```{r RandomForest ROC model, echo=FALSE, message=FALSE, warning=FALSE}
rfFit2<- train(Credit.Application.Result ~.,
              method = 'rf',
              trControl = fitControl2,
              tuneLength = 10,
              metric ='ROC',
              data = train_set)

rfFit2

```
_Table 4.5 Random Forest Model output (metric = Accuracy)_


```{r RF ROC plot, echo=FALSE}
plot(rfFit2, main= "Random Forest - ROC plot")
```
_Fig 4.7 Random Forest - ROC plot_

__Random Forest Test set prediction and confusion matrix__
Next we applied the model with accuracy as metric of choice to the hold-out sample. This gave an overall accuracy of 78.52%. This model performed at par with the Decision Tree which also had an accuracy of 78.52%, but did not perform better than the Logistic Regression.However, we this model had a higher percentage for the NPV with a lesser difference from the PPV. This is particularly important as the bank needs to ensure they do not grant loans to Non-creditworthy customers, while ensuring that they adequately harness the opportunity not wrongly classifying Creditworthy customers in the process.Based on this we can conclude that there is no bias towards any of the classes.

```{r RF test prediction and CM, echo=FALSE, message=FALSE, warning=FALSE}
##Confusion matrix
rf.pred <- predict(rfFit,test_set)
cm3<-confusionMatrix(table(rf.pred,test_set$Credit.Application.Result))   
cm3
```
_Table 4.6 Random Forest Confusion Matrix_

__4. Boosted Tree Model__

This optimal model had an accuracy of 74.04.% with an n.trees of 134. See the details for the optimal parameter in table 4.7 below. The Accuracy plot is also seen in Fig 4.8.
```{r Boosted with Accuracy, echo=FALSE, message=FALSE, warning=FALSE}
gbmFit<- train(Credit.Application.Result ~., 
               method = 'gbm',
               trControl = fitControl,
               tuneGrid = tune.Grid,
               verbose = FALSE,
               data = train_set)


gbmFit$bestTune
```
_Table 4.7 Boosted Model optimal performance parameters_

```{r Boosted Accuracy plot, echo=FALSE, message=FALSE, warning=FALSE}
plot(gbmFit, main = "Boosted Tree, Accuracy Plot")
```
_Fig 4.8 Boosted Model Accuracy Plot

__Boosted Model variable importance__  

This model introduced ranked the top three variables by importance as _Credit.Amount_ , _Account.Balance_ , and _Age.years_. This is a bit of a depature from the Decision Tree and Random Forest that had the same variables but in different order.

```{r Boosted Var Imp Plot, echo=FALSE, message=FALSE, warning=FALSE}
plot(varImp(gbmFit))
```
_Fig 4.9 Boosted Model Variable Importance plot_

__Boosted Model with ROC as metric__
The ROC metric gave us an optimal performance with an ROC of 72% with n.tree value of 94. See
```{r Boosted model by ROC metric, echo=FALSE, message=FALSE, warning=FALSE}
gbmFit2<- train(Credit.Application.Result ~., 
               method = 'gbm',
               trControl = fitControl2,
               tuneGrid = tune.Grid,
               metric ='ROC',
               verbose = FALSE,
               data = train_set)

gbmFit2$bestTune
```
_Table 4.8 Boosted Model ROC optimal performance parameters_


```{r Boosted ROC plot, echo=FALSE}
plot(gbmFit2, main = "Boosted Model ROC plot")
```
_Fig 4.9 Boosted model ROC plot_

__Boosted Model test prediction and confusion matrix__  
The boosted model had an overall accuracy of 77.18% and the calculated values for the PPV and NPV revealed that it is biased towards the Creditworthy class.

```{r Boosted test prediction and CM, echo=FALSE, message=FALSE, warning=FALSE}
gbm.pred <- predict(gbmFit,test_set)
cm4<-confusionMatrix(table(gbm.pred,test_set$Credit.Application.Result))     
cm4
```
_Table 4.9 Boosted model confusion matrix output_


## Section 5: Model Performance Metrics Evaluation

Before we chose the model to use for prediction, we evaluated the performance of the 4 algorithms. We did a resampling of all the models, summarizing the results and plotting boxplots to visualize their performance metrics.

```{r Comparing Models by Accuracy, echo=FALSE, message=FALSE, warning=FALSE}
## Comparing the models
#Accuracy
fit.compare <-resamples(list(LogisticRegression = glmFit,Tree =treeFit,
                             Forest=rfFit, Boosted =gbmFit))
summary(fit.compare)

```
_Table 5.1 Accuracy metric model evaluation output_


```{r echo=FALSE, message=FALSE, warning=FALSE}
scales <- list(x=list(relation="free"), y=list(relation="free"))
bwplot(fit.compare, scales=scales)
```
_Fig 5.2 Accuracy metric evaluation boxplots_

__Analysis of Accuracy performance metric for the 4 algorithms__  
Earlier in section 4 the Logistic Regression had the highest model accuracy on test prediction.The summary of a 100 resamples in the output of our analysis in Table 5.1 above shows the Forest Model has the highest mean Accuracy performance of 75.47%.

```{r Comparing models by ROC, echo=FALSE, message=FALSE, warning=FALSE}
#ROC
fit.compare2 <-resamples(list(LogisticRegression = glmFit2,Tree =treeFit2,
                             Forest=rfFit2, Boosted =gbmFit2))
summary(fit.compare2)
```
_Table 5.2 ROC performance metric evaluation output_

__Analysis of ROC performance metric for the 4 algorithms__  
The summary output of the resamples shows that the Forest Model performed best with a mean ROC of 75.32% which is consistent with its position in section 4 ranking 1st with an optimal performance ROC of 74.91%. This can also be seen in the the boxplot in Fig 5.2.

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Draw box plots to compare models
scales <- list(x=list(relation="free"), y=list(relation="free"))
bwplot(fit.compare2,scales=scales)
```


__Summary of Basis for Selecting Random Forest for Prediction__  
The comparison revealed that the __Forest Model__ performed better than the other 3 models based on the foregoing:   
* the model had the highest accuracy of 75.82% with 10 fold repeated cross validation  
* the model the best optimal performance with an ROC metric of 74.91%    
* the model has the highest mean accuracy of 75.47% over 100 samples  
* the model has the highest mean ROC of 75.32% over a 100 samples and slightly better than the in-sample model  
* the model has the ability to predict correctly more Non-creditworthy application with an NPV of 75%. This is particularly important as the bank needs to ensure they do not grant loans to Non-creditworthy customers, while ensuring that they adequately harness the opportunity of not wrongly classifying Creditworthy customers in the process.













